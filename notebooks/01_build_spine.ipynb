{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Data Cleaning & Ground-Truth Panel (LOCKED)\n",
    "\n",
    "**Status: COMPLETE & LOCKED**\n",
    "\n",
    "---\n",
    "\n",
    "## Cleaning Rules (Final)\n",
    "\n",
    "| Rule | Implementation | Purpose |\n",
    "|------|----------------|----------|\n",
    "| Negative sales | `sales_clean = max(saleqty, 0)` | Clip to 0, `was_negative` flag for audit |\n",
    "| COVID period | `is_covid_period = 1` for [2020-03-15, 2021-06-30] | **Diagnostic only** |\n",
    "| COVID panic spike | `is_covid_panic_spike = 1` for COVID + sales > p99.9 | **For downweight/cap** |\n",
    "| Extreme spike | `is_extreme_spike = 1` for sales > 10,000 | Flag only (valid demand) |\n",
    "| Data outages | Calendar features | No target manipulation |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.1 — Install libraries (one-time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install --upgrade google-cloud-bigquery google-cloud-storage pandas pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.2 — Set config variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"myforecastingsales\"\n",
    "DATASET_ID = \"forecasting\"\n",
    "LOCATION   = \"me-central1\"\n",
    "BUCKET     = \"myforecastingsales-data\"\n",
    "\n",
    "GCS_SALES_URI = \"gs://myforecastingsales-data/raw/sales/final_data.csv\"\n",
    "GCS_ATTR_URI  = \"gs://myforecastingsales-data/raw/sku_attributes/sku_list_attribute.csv\"\n",
    "\n",
    "# Table names\n",
    "SALES_RAW_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.sales_raw\"\n",
    "SKU_ATTR_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.sku_attr\"\n",
    "SALES_DAILY_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.sales_daily\"\n",
    "SALES_DAILY_CLEAN_TABLE = f\"{PROJECT_ID}.{DATASET_ID}.sales_daily_clean\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.3 — Create BigQuery client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "bq = bigquery.Client(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "# Sanity check: list datasets\n",
    "print(\"Datasets:\", [d.dataset_id for d in bq.list_datasets(PROJECT_ID)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.4 — Load CSVs from GCS into BigQuery\n",
    "\n",
    "### 1.4A Load sales_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=True,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    "    allow_quoted_newlines=True,\n",
    ")\n",
    "\n",
    "load_job = bq.load_table_from_uri(GCS_SALES_URI, SALES_RAW_TABLE, job_config=job_config)\n",
    "load_job.result()\n",
    "\n",
    "print(f\"✓ sales_raw: {bq.get_table(SALES_RAW_TABLE).num_rows:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4B Load sku_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig(\n",
    "    source_format=bigquery.SourceFormat.CSV,\n",
    "    skip_leading_rows=1,\n",
    "    autodetect=True,\n",
    "    write_disposition=\"WRITE_TRUNCATE\",\n",
    ")\n",
    "\n",
    "load_job = bq.load_table_from_uri(GCS_ATTR_URI, SKU_ATTR_TABLE, job_config=job_config)\n",
    "load_job.result()\n",
    "\n",
    "print(f\"✓ sku_attr: {bq.get_table(SKU_ATTR_TABLE).num_rows:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5 — Build sales_daily (deduplicated)\n",
    "\n",
    "Aggregate at (store_id, sku_id, date) level. Preserve raw values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{SALES_DAILY_TABLE}` AS\n",
    "SELECT\n",
    "  CAST(store_id AS STRING) AS store_id,\n",
    "  CAST(item_id AS STRING) AS sku_id,\n",
    "  DATE(date) AS date,\n",
    "  CAST(SUM(sales) AS INT64) AS saleqty\n",
    "FROM `{SALES_RAW_TABLE}`\n",
    "GROUP BY store_id, item_id, date\n",
    "\"\"\"\n",
    "bq.query(sql).result()\n",
    "\n",
    "print(f\"✓ sales_daily: {bq.get_table(SALES_DAILY_TABLE).num_rows:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.6 — Build sales_daily_clean (LOCKED CLEANING RULES)\n",
    "\n",
    "**Cleaning Contract:**\n",
    "1. `sales_clean = max(saleqty, 0)` — negatives clipped to 0\n",
    "2. `was_negative = 1` — audit flag for clipped rows\n",
    "3. `is_covid_period = 1` — for dates in [2020-03-15, 2021-06-30] **(DIAGNOSTIC ONLY)**\n",
    "4. `is_covid_panic_spike = 1` — COVID period + sales > global p99.9 **(FOR DOWNWEIGHT/CAP)**\n",
    "5. `is_extreme_spike = 1` — for sales > 10,000 (flagged only, NOT clipped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{SALES_DAILY_CLEAN_TABLE}` AS\n",
    "WITH global_stats AS (\n",
    "  SELECT APPROX_QUANTILES(saleqty, 10000)[OFFSET(9990)] AS p99_9\n",
    "  FROM `{SALES_DAILY_TABLE}`\n",
    "  WHERE saleqty > 0\n",
    ")\n",
    "SELECT\n",
    "  store_id,\n",
    "  sku_id,\n",
    "  date,\n",
    "  saleqty AS sales_raw,\n",
    "  \n",
    "  -- Rule 1: Negative sales clipped to 0\n",
    "  GREATEST(saleqty, 0) AS sales_clean,\n",
    "  IF(saleqty < 0, 1, 0) AS was_negative,\n",
    "  \n",
    "  -- Rule 2a: COVID period flag (DIAGNOSTIC ONLY)\n",
    "  IF(date BETWEEN \"2020-03-15\" AND \"2021-06-30\", 1, 0) AS is_covid_period,\n",
    "  \n",
    "  -- Rule 2b: COVID panic spike (FOR DOWNWEIGHT/CAP)\n",
    "  -- COVID period + sales > global p99.9\n",
    "  IF(date BETWEEN \"2020-03-15\" AND \"2021-06-30\" AND saleqty > (SELECT p99_9 FROM global_stats), 1, 0) AS is_covid_panic_spike,\n",
    "  \n",
    "  -- Rule 3: Extreme spike (absolute threshold >10,000)\n",
    "  IF(saleqty > 10000, 1, 0) AS is_extreme_spike\n",
    "\n",
    "FROM `{SALES_DAILY_TABLE}`, global_stats\n",
    "\"\"\"\n",
    "bq.query(sql).result()\n",
    "\n",
    "print(f\"✓ sales_daily_clean: {bq.get_table(SALES_DAILY_CLEAN_TABLE).num_rows:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.7 — Verify Cleaning Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  COUNT(*) AS total_rows,\n",
    "  SUM(was_negative) AS was_negative_count,\n",
    "  ROUND(100.0 * SUM(was_negative) / COUNT(*), 4) AS was_negative_pct,\n",
    "  SUM(is_covid_period) AS is_covid_period_count,\n",
    "  ROUND(100.0 * SUM(is_covid_period) / COUNT(*), 2) AS is_covid_period_pct,\n",
    "  SUM(is_covid_panic_spike) AS is_covid_panic_spike_count,\n",
    "  ROUND(100.0 * SUM(is_covid_panic_spike) / COUNT(*), 4) AS is_covid_panic_spike_pct,\n",
    "  SUM(is_extreme_spike) AS is_extreme_spike_count,\n",
    "  ROUND(100.0 * SUM(is_extreme_spike) / COUNT(*), 4) AS is_extreme_spike_pct,\n",
    "  MIN(date) AS min_date,\n",
    "  MAX(date) AS max_date\n",
    "FROM `{SALES_DAILY_CLEAN_TABLE}`\n",
    "\"\"\"\n",
    "result = list(bq.query(sql).result())[0]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1 CLEANING VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total rows:              {result.total_rows:,}\")\n",
    "print(f\"Date range:              {result.min_date} → {result.max_date}\")\n",
    "print(f\"was_negative:            {result.was_negative_count:,} ({result.was_negative_pct}%)\")\n",
    "print(f\"is_covid_period:         {result.is_covid_period_count:,} ({result.is_covid_period_pct}%) [diagnostic]\")\n",
    "print(f\"is_covid_panic_spike:    {result.is_covid_panic_spike_count:,} ({result.is_covid_panic_spike_pct}%) [downweight]\")\n",
    "print(f\"is_extreme_spike:        {result.is_extreme_spike_count:,} ({result.is_extreme_spike_pct}%)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.8 — Quality Gates (NON-NEGOTIABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gate 1: UNIQUE KEYS (rows == distinct(store_id, sku_id, date))\n",
    "sql = f\"\"\"\n",
    "SELECT \n",
    "  COUNT(*) AS total_rows, \n",
    "  COUNT(DISTINCT CONCAT(store_id, '|', sku_id, '|', CAST(date AS STRING))) AS distinct_keys\n",
    "FROM `{SALES_DAILY_CLEAN_TABLE}`\n",
    "\"\"\"\n",
    "result = list(bq.query(sql).result())[0]\n",
    "assert result.total_rows == result.distinct_keys, f\"FAIL: Duplicate keys! {result.total_rows} != {result.distinct_keys}\"\n",
    "print(f\"✓ Gate 1 PASS: UNIQUE KEYS ({result.total_rows:,} rows = {result.distinct_keys:,} distinct keys)\")\n",
    "\n",
    "# Gate 2: No nulls in key columns\n",
    "sql = f\"\"\"\n",
    "SELECT COUNTIF(store_id IS NULL OR sku_id IS NULL OR date IS NULL) AS null_keys\n",
    "FROM `{SALES_DAILY_CLEAN_TABLE}`\n",
    "\"\"\"\n",
    "result = list(bq.query(sql).result())[0]\n",
    "assert result.null_keys == 0, \"FAIL: Null keys found!\"\n",
    "print(f\"✓ Gate 2 PASS: No null keys\")\n",
    "\n",
    "# Gate 3: sales_clean >= 0 (negatives clipped)\n",
    "sql = f\"\"\"\n",
    "SELECT COUNTIF(sales_clean < 0) AS negative_clean, MIN(sales_clean) AS min_clean\n",
    "FROM `{SALES_DAILY_CLEAN_TABLE}`\n",
    "\"\"\"\n",
    "result = list(bq.query(sql).result())[0]\n",
    "assert result.negative_clean == 0, \"FAIL: Negative sales_clean found!\"\n",
    "print(f\"✓ Gate 3 PASS: All sales_clean >= 0 (min = {result.min_clean})\")\n",
    "\n",
    "# Gate 4: Top 10 sales all have is_extreme_spike=1\n",
    "sql = f\"\"\"\n",
    "SELECT sales_clean, is_extreme_spike\n",
    "FROM `{SALES_DAILY_CLEAN_TABLE}`\n",
    "ORDER BY sales_clean DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "top10 = list(bq.query(sql).result())\n",
    "all_flagged = all(row.is_extreme_spike == 1 for row in top10)\n",
    "assert all_flagged, \"FAIL: Top 10 sales don't all have is_extreme_spike=1!\"\n",
    "print(f\"✓ Gate 4 PASS: Top 10 sales all have is_extreme_spike=1\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ALL QUALITY GATES PASSED\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.9 — COVID Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  CASE WHEN is_covid_period = 1 THEN 'COVID (2020-03-15 to 2021-06-30)' ELSE 'Non-COVID' END AS period,\n",
    "  COUNT(*) AS row_count,\n",
    "  ROUND(AVG(sales_clean), 2) AS avg_sales,\n",
    "  ROUND(STDDEV(sales_clean), 2) AS std_sales,\n",
    "  SUM(is_covid_panic_spike) AS panic_spikes,\n",
    "  SUM(is_extreme_spike) AS extreme_spikes\n",
    "FROM `{SALES_DAILY_CLEAN_TABLE}`\n",
    "GROUP BY is_covid_period\n",
    "ORDER BY is_covid_period\n",
    "\"\"\"\n",
    "print(\"COVID vs Non-COVID Comparison:\")\n",
    "for row in bq.query(sql).result():\n",
    "    print(f\"  {row.period}: {row.row_count:,} rows, avg={row.avg_sales}, panic_spikes={row.panic_spikes}, extreme={row.extreme_spikes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.10 — Negatives Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  CASE WHEN was_negative = 1 THEN 'Was Negative (now 0)' ELSE 'Original >= 0' END AS category,\n",
    "  COUNT(*) AS row_count,\n",
    "  ROUND(AVG(sales_raw), 2) AS avg_raw,\n",
    "  MIN(sales_raw) AS min_raw,\n",
    "  MAX(sales_raw) AS max_raw\n",
    "FROM `{SALES_DAILY_CLEAN_TABLE}`\n",
    "GROUP BY was_negative\n",
    "\"\"\"\n",
    "print(\"Negatives Analysis:\")\n",
    "for row in bq.query(sql).result():\n",
    "    print(f\"  {row.category}: {row.row_count:,} rows, avg_raw={row.avg_raw}, range=[{row.min_raw}, {row.max_raw}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.11 — Create Cleaning Report Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f\"\"\"\n",
    "CREATE OR REPLACE TABLE `{PROJECT_ID}.{DATASET_ID}.step1_cleaning_report` AS\n",
    "SELECT\n",
    "  \"Step 1 Cleaning Report\" AS report_name,\n",
    "  CURRENT_TIMESTAMP() AS generated_at,\n",
    "  (SELECT COUNT(*) FROM `{SALES_DAILY_CLEAN_TABLE}`) AS total_rows,\n",
    "  (SELECT SUM(was_negative) FROM `{SALES_DAILY_CLEAN_TABLE}`) AS negative_rows_cleaned,\n",
    "  (SELECT SUM(is_covid_period) FROM `{SALES_DAILY_CLEAN_TABLE}`) AS covid_period_rows,\n",
    "  (SELECT SUM(is_covid_panic_spike) FROM `{SALES_DAILY_CLEAN_TABLE}`) AS covid_panic_spike_rows,\n",
    "  (SELECT SUM(is_extreme_spike) FROM `{SALES_DAILY_CLEAN_TABLE}`) AS extreme_spike_rows,\n",
    "  (SELECT MIN(date) FROM `{SALES_DAILY_CLEAN_TABLE}`) AS data_start_date,\n",
    "  (SELECT MAX(date) FROM `{SALES_DAILY_CLEAN_TABLE}`) AS data_end_date,\n",
    "  DATE(\"2020-03-15\") AS covid_start,\n",
    "  DATE(\"2021-06-30\") AS covid_end,\n",
    "  \"max(saleqty, 0)\" AS negative_handling,\n",
    "  \"is_covid_period (diagnostic) + is_covid_panic_spike (downweight/cap)\" AS covid_handling,\n",
    "  \"kept as valid demand\" AS spike_handling,\n",
    "  \"calendar features\" AS outage_handling\n",
    "\"\"\"\n",
    "bq.query(sql).result()\n",
    "print(\"✓ step1_cleaning_report created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Data Lineage\n",
    "```\n",
    "GCS: final_data.csv\n",
    "    ↓\n",
    "sales_raw (original, untouched)\n",
    "    ↓\n",
    "sales_daily (deduplicated at store×sku×date)\n",
    "    ↓\n",
    "sales_daily_clean (FINAL - with cleaning flags)\n",
    "```\n",
    "\n",
    "### BigQuery Tables\n",
    "| Table | Description |\n",
    "|-------|-------------|\n",
    "| `sales_raw` | Original CSV loaded, untouched |\n",
    "| `sku_attr` | SKU attributes |\n",
    "| `sales_daily` | Deduplicated daily sales |\n",
    "| `sales_daily_clean` | **Cleaned data with flags** |\n",
    "| `step1_cleaning_report` | Cleaning rules documentation |\n",
    "\n",
    "### Cleaning Flags in `sales_daily_clean`\n",
    "| Column | Description | Purpose |\n",
    "|--------|-------------|----------|\n",
    "| `sales_raw` | Original value (can be negative) | Audit |\n",
    "| `sales_clean` | Cleaned value (≥0) | Model target |\n",
    "| `was_negative` | 1 if original was negative | Audit |\n",
    "| `is_covid_period` | 1 if date in [2020-03-15, 2021-06-30] | **Diagnostic only** |\n",
    "| `is_covid_panic_spike` | 1 if COVID + sales > p99.9 | **For downweight/cap** |\n",
    "| `is_extreme_spike` | 1 if sales > 10,000 | Flag extreme values |\n",
    "\n",
    "### Quality Gates (All Passed)\n",
    "1. ✅ UNIQUE KEYS: rows == distinct(store_id, sku_id, date)\n",
    "2. ✅ No null keys\n",
    "3. ✅ All sales_clean >= 0\n",
    "4. ✅ Top 10 sales all have is_extreme_spike=1\n",
    "\n",
    "### Flag Counts\n",
    "| Flag | Count | % |\n",
    "|------|-------|---|\n",
    "| `was_negative` | 30,863 | 0.05% |\n",
    "| `is_covid_period` | 7,021,480 | 11.51% |\n",
    "| `is_covid_panic_spike` | 7,609 | 0.0125% |\n",
    "| `is_extreme_spike` | 139 | 0.0002% |\n",
    "\n",
    "---\n",
    "\n",
    "**Step 1 is now COMPLETE and LOCKED.**\n",
    "\n",
    "**Next Step:** Step 2 - Feature Engineering"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

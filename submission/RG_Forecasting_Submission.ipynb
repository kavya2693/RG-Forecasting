{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RG-Forecasting: 24-Week Retail Demand Forecast\n",
    "\n",
    "**Store-SKU Level Daily Predictions | 33 Stores | ~3,650 SKUs | 168-Day Horizon**\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Data Period | 2019 - Dec 17, 2025 |\n",
    "| Forecast Horizon | Dec 18, 2025 - Jun 3, 2026 (168 days) |\n",
    "| Series Forecasted | ~114,000 store-SKU combinations |\n",
    "| Model | Two-Stage LightGBM (Classifier + Log-Regressor) |\n",
    "| Weekly Store Accuracy | ~80-88% WFA |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Configuration](#1-configuration)\n",
    "2. [Setup & Imports](#2-setup)\n",
    "3. [Data Loading & Validation](#3-data-loading)\n",
    "4. [Exploratory Data Analysis](#4-eda)\n",
    "5. [Panel Construction (Spine)](#5-panel)\n",
    "6. [Data Cleaning](#6-cleaning)\n",
    "7. [Feature Engineering](#7-features)\n",
    "8. [ABC Segmentation](#8-segmentation)\n",
    "9. [Model Training](#9-training)\n",
    "10. [Forecast Generation](#10-forecast)\n",
    "11. [Evaluation & Sanity Checks](#11-evaluation)\n",
    "12. [Output & Submission Checklist](#12-output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Configuration <a name=\"1-configuration\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Update these paths for your environment\n",
    "\n",
    "DATA_PATH = \"../final_data 2.csv\"              # Raw sales transaction data\n",
    "SKU_ATTR_PATH = \"../sku_list_attribute.csv\"    # SKU attributes (local/import)\n",
    "OUTPUT_PATH = \"outputs/forecast_168day.csv\"    # Forecast output\n",
    "\n",
    "# Date configuration\n",
    "CUTOFF_DATE = \"2025-12-17\"       # Last date of training data\n",
    "FORECAST_START = \"2025-12-18\"    # First forecast date  \n",
    "HORIZON_DAYS = 168               # 24 weeks = 168 days\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Performance options\n",
    "SAMPLE_MODE = False    # Set True for quick testing (uses 10% of SKUs)\n",
    "SAMPLE_FRAC = 0.1      # Fraction of SKUs to use in sample mode\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data: {DATA_PATH}\")\n",
    "print(f\"  Cutoff: {CUTOFF_DATE}\")\n",
    "print(f\"  Forecast: {FORECAST_START} + {HORIZON_DAYS} days\")\n",
    "print(f\"  Sample mode: {SAMPLE_MODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setup & Imports <a name=\"2-setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
    "print(f\"\\nEnvironment:\")\n",
    "print(f\"  Python: {pd.sys.version.split()[0]}\")\n",
    "print(f\"  Pandas: {pd.__version__}\")\n",
    "print(f\"  NumPy: {np.__version__}\")\n",
    "print(f\"  LightGBM: {lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading & Validation <a name=\"3-data-loading\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading raw sales data...\")\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(f\"  Loaded {len(df_raw):,} rows\")\n",
    "\n",
    "# Standardize column names\n",
    "df_raw.columns = df_raw.columns.str.lower().str.strip()\n",
    "if 'item_id' in df_raw.columns:\n",
    "    df_raw = df_raw.rename(columns={'item_id': 'sku_id'})\n",
    "\n",
    "# Parse date\n",
    "df_raw['date'] = pd.to_datetime(df_raw['date'])\n",
    "\n",
    "print(f\"\\nSchema validation:\")\n",
    "print(f\"  Columns: {list(df_raw.columns)}\")\n",
    "assert 'sku_id' in df_raw.columns, \"Missing sku_id column\"\n",
    "assert 'store_id' in df_raw.columns, \"Missing store_id column\"\n",
    "assert 'date' in df_raw.columns, \"Missing date column\"\n",
    "assert 'sales' in df_raw.columns, \"Missing sales column\"\n",
    "print(\"  ‚úì All required columns present\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SKU attributes (Local vs Import)\n",
    "print(\"Loading SKU attributes...\")\n",
    "sku_attr = pd.read_csv(SKU_ATTR_PATH)\n",
    "sku_attr.columns = sku_attr.columns.str.lower().str.strip()\n",
    "if 'item_id' in sku_attr.columns:\n",
    "    sku_attr = sku_attr.rename(columns={'item_id': 'sku_id'})\n",
    "\n",
    "# Create is_local flag\n",
    "attr_col = [c for c in sku_attr.columns if 'attribute' in c.lower() or 'local' in c.lower()][0]\n",
    "sku_attr['is_local'] = sku_attr[attr_col].apply(lambda x: 1 if str(x).upper() in ['L', 'LI', 'LOCAL'] else 0)\n",
    "sku_attr['sku_id'] = sku_attr['sku_id'].astype(str)\n",
    "\n",
    "print(f\"  Loaded {len(sku_attr):,} SKU attributes\")\n",
    "print(f\"  Local: {sku_attr['is_local'].sum():,}, Import: {(1-sku_attr['is_local']).sum():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data summary\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Date range: {df_raw['date'].min().date()} to {df_raw['date'].max().date()}\")\n",
    "print(f\"Unique stores: {df_raw['store_id'].nunique()}\")\n",
    "print(f\"Unique SKUs: {df_raw['sku_id'].nunique()}\")\n",
    "print(f\"Total transactions: {len(df_raw):,}\")\n",
    "print(f\"Total sales volume: {df_raw['sales'].sum():,.0f} units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample mode: reduce to subset of SKUs for faster testing\n",
    "if SAMPLE_MODE:\n",
    "    print(f\"\\n‚ö†Ô∏è SAMPLE MODE ENABLED: Using {SAMPLE_FRAC*100:.0f}% of SKUs\")\n",
    "    all_skus = df_raw['sku_id'].unique()\n",
    "    sample_skus = np.random.choice(all_skus, size=int(len(all_skus) * SAMPLE_FRAC), replace=False)\n",
    "    df_raw = df_raw[df_raw['sku_id'].isin(sample_skus)]\n",
    "    print(f\"  Reduced to {len(df_raw):,} rows, {len(sample_skus):,} SKUs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis <a name=\"4-eda\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THE KEY CHALLENGE: Sparsity\n",
    "# Transaction data only contains days WITH sales\n",
    "# Missing dates = zero sales (not missing data)\n",
    "\n",
    "# Calculate theoretical complete panel size\n",
    "n_stores = df_raw['store_id'].nunique()\n",
    "n_skus = df_raw['sku_id'].nunique()\n",
    "n_days = (df_raw['date'].max() - df_raw['date'].min()).days + 1\n",
    "theoretical_rows = n_stores * n_skus * n_days\n",
    "actual_rows = len(df_raw)\n",
    "density = actual_rows / theoretical_rows * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SPARSITY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Theoretical panel size: {n_stores} stores √ó {n_skus:,} SKUs √ó {n_days:,} days\")\n",
    "print(f\"                      = {theoretical_rows:,} rows\")\n",
    "print(f\"Actual transactions:    {actual_rows:,} rows\")\n",
    "print(f\"Data density:           {density:.1f}%\")\n",
    "print(f\"\\nüî¥ ZERO RATE: ~{100-density:.0f}% of store-SKU-days have zero sales\")\n",
    "print(\"   This drives our two-stage modeling approach.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# All sales (clipped for visualization)\n",
    "df_raw['sales'].clip(upper=50).hist(bins=50, ax=axes[0], color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Daily Sales Distribution (clipped at 50)')\n",
    "axes[0].set_xlabel('Daily Sales (units)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "# Sales percentiles\n",
    "percentiles = [50, 75, 90, 95, 99, 100]\n",
    "pct_values = [df_raw['sales'].quantile(p/100) for p in percentiles]\n",
    "axes[1].bar([str(p) for p in percentiles], pct_values, color='coral', edgecolor='white')\n",
    "axes[1].set_title('Sales Percentiles')\n",
    "axes[1].set_xlabel('Percentile')\n",
    "axes[1].set_ylabel('Sales (units)')\n",
    "for i, v in enumerate(pct_values):\n",
    "    axes[1].text(i, v + 1, f'{v:.0f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Day-of-week and monthly patterns\n",
    "df_raw['dow'] = df_raw['date'].dt.dayofweek\n",
    "df_raw['month'] = df_raw['date'].dt.month\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Day of week\n",
    "dow_sales = df_raw.groupby('dow')['sales'].mean()\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[0].bar(dow_names, dow_sales.values, color='steelblue', edgecolor='white')\n",
    "axes[0].set_title('Average Sales by Day of Week')\n",
    "axes[0].set_ylabel('Avg Sales (units)')\n",
    "\n",
    "# Monthly\n",
    "monthly_sales = df_raw.groupby('month')['sales'].mean()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "colors = ['coral' if m == 12 else 'steelblue' for m in range(1, 13)]\n",
    "axes[1].bar(month_names, monthly_sales.values, color=colors, edgecolor='white')\n",
    "axes[1].set_title('Average Sales by Month')\n",
    "axes[1].set_ylabel('Avg Sales (units)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key patterns:\")\n",
    "print(f\"  - Weekend effect: Sun/Mon show distinct patterns\")\n",
    "print(f\"  - December lift: {(monthly_sales.iloc[11] / monthly_sales.iloc[:11].mean() - 1) * 100:.0f}% above average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Panel Construction (Spine) <a name=\"5-panel\"></a>\n",
    "\n",
    "Create a complete store-SKU-date grid and fill missing dates with zero sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building complete panel (spine)...\")\n",
    "print(\"  This fills missing dates with zero sales.\")\n",
    "\n",
    "# Get unique combinations\n",
    "stores = df_raw['store_id'].unique()\n",
    "skus = df_raw['sku_id'].unique()\n",
    "\n",
    "# Date range for training (up to cutoff)\n",
    "cutoff = pd.to_datetime(CUTOFF_DATE)\n",
    "min_date = df_raw['date'].min()\n",
    "date_range = pd.date_range(min_date, cutoff, freq='D')\n",
    "\n",
    "print(f\"  Stores: {len(stores)}\")\n",
    "print(f\"  SKUs: {len(skus):,}\")\n",
    "print(f\"  Dates: {len(date_range):,} ({min_date.date()} to {cutoff.date()})\")\n",
    "print(f\"  Expected panel size: {len(stores) * len(skus) * len(date_range):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build spine efficiently using merge\n",
    "print(\"\\nGenerating spine...\")\n",
    "\n",
    "# Get all store-SKU combinations that ever had a sale\n",
    "series = df_raw[['store_id', 'sku_id']].drop_duplicates()\n",
    "print(f\"  Unique series: {len(series):,}\")\n",
    "\n",
    "# Create date dataframe\n",
    "dates_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Cross join series with dates\n",
    "series['_key'] = 1\n",
    "dates_df['_key'] = 1\n",
    "spine = series.merge(dates_df, on='_key').drop('_key', axis=1)\n",
    "\n",
    "print(f\"  Spine size: {len(spine):,} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge sales onto spine (missing = 0)\n",
    "print(\"\\nMerging sales onto spine...\")\n",
    "\n",
    "# Prepare sales data (up to cutoff)\n",
    "df_train = df_raw[df_raw['date'] <= cutoff][['store_id', 'sku_id', 'date', 'sales']].copy()\n",
    "df_train = df_train.rename(columns={'sales': 'y'})\n",
    "\n",
    "# Merge\n",
    "panel = spine.merge(df_train, on=['store_id', 'sku_id', 'date'], how='left')\n",
    "panel['y'] = panel['y'].fillna(0)\n",
    "\n",
    "# Verify\n",
    "zero_rate = (panel['y'] == 0).mean() * 100\n",
    "print(f\"  Panel size: {len(panel):,} rows\")\n",
    "print(f\"  Zero-sales rate: {zero_rate:.1f}%\")\n",
    "\n",
    "# Clean up\n",
    "del spine, df_train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Cleaning <a name=\"6-cleaning\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleaning data...\")\n",
    "\n",
    "# 1. Clip negative sales to 0 (returns)\n",
    "neg_count = (panel['y'] < 0).sum()\n",
    "panel['y'] = panel['y'].clip(lower=0)\n",
    "print(f\"  Clipped {neg_count:,} negative values to 0\")\n",
    "\n",
    "# 2. Convert IDs to string for categorical handling\n",
    "panel['store_id'] = panel['store_id'].astype(str)\n",
    "panel['sku_id'] = panel['sku_id'].astype(str)\n",
    "\n",
    "# 3. Merge SKU attributes (Local vs Import)\n",
    "panel = panel.merge(sku_attr[['sku_id', 'is_local']], on='sku_id', how='left')\n",
    "panel['is_local'] = panel['is_local'].fillna(0).astype(int)\n",
    "\n",
    "local_pct = panel.groupby('sku_id')['is_local'].first().mean() * 100\n",
    "print(f\"  SKU attribute: {local_pct:.1f}% Local, {100-local_pct:.1f}% Import\")\n",
    "\n",
    "print(\"\\n‚úì Cleaning complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Engineering <a name=\"7-features\"></a>\n",
    "\n",
    "All features are **causal**: computed using only past data relative to the prediction date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Engineering features...\")\n",
    "print(\"  All features use only past data (no leakage).\")\n",
    "\n",
    "# Sort for proper lag calculation\n",
    "panel = panel.sort_values(['store_id', 'sku_id', 'date']).reset_index(drop=True)\n",
    "\n",
    "# === CALENDAR FEATURES ===\n",
    "panel['dow'] = panel['date'].dt.dayofweek\n",
    "panel['is_weekend'] = panel['dow'].isin([5, 6]).astype(int)\n",
    "panel['week_of_year'] = panel['date'].dt.isocalendar().week.astype(int)\n",
    "panel['month'] = panel['date'].dt.month\n",
    "panel['day_of_year'] = panel['date'].dt.dayofyear\n",
    "\n",
    "# Cyclical encoding\n",
    "panel['sin_doy'] = np.sin(2 * np.pi * panel['day_of_year'] / 365)\n",
    "panel['cos_doy'] = np.cos(2 * np.pi * panel['day_of_year'] / 365)\n",
    "panel['sin_dow'] = np.sin(2 * np.pi * panel['dow'] / 7)\n",
    "panel['cos_dow'] = np.cos(2 * np.pi * panel['dow'] / 7)\n",
    "\n",
    "print(\"  ‚úì Calendar features (9)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LAG FEATURES ===\n",
    "# Grouped by series for proper lag calculation\n",
    "print(\"  Computing lag features...\")\n",
    "\n",
    "for lag in [1, 7, 14, 28, 56]:\n",
    "    panel[f'lag_{lag}'] = panel.groupby(['store_id', 'sku_id'])['y'].shift(lag)\n",
    "\n",
    "print(\"  ‚úì Lag features (5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ROLLING FEATURES ===\n",
    "print(\"  Computing rolling features...\")\n",
    "\n",
    "# Use shift(1) to avoid including current day\n",
    "for window in [7, 28]:\n",
    "    rolled = panel.groupby(['store_id', 'sku_id'])['y'].transform(\n",
    "        lambda x: x.shift(1).rolling(window, min_periods=1).mean()\n",
    "    )\n",
    "    panel[f'roll_mean_{window}'] = rolled\n",
    "    \n",
    "    rolled_sum = panel.groupby(['store_id', 'sku_id'])['y'].transform(\n",
    "        lambda x: x.shift(1).rolling(window, min_periods=1).sum()\n",
    "    )\n",
    "    panel[f'roll_sum_{window}'] = rolled_sum\n",
    "\n",
    "# Rolling std\n",
    "panel['roll_std_28'] = panel.groupby(['store_id', 'sku_id'])['y'].transform(\n",
    "    lambda x: x.shift(1).rolling(28, min_periods=7).std()\n",
    ").fillna(0)\n",
    "\n",
    "print(\"  ‚úì Rolling features (5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DORMANCY FEATURES ===\n",
    "print(\"  Computing dormancy features...\")\n",
    "\n",
    "# Non-zero rate in last 28 days\n",
    "panel['nz_rate_28'] = panel.groupby(['store_id', 'sku_id'])['y'].transform(\n",
    "    lambda x: x.shift(1).rolling(28, min_periods=1).apply(lambda w: (w > 0).mean())\n",
    ").fillna(0)\n",
    "\n",
    "# Days since last sale (capped at 90)\n",
    "def days_since_last_sale(series):\n",
    "    result = np.zeros(len(series))\n",
    "    last_sale_idx = -1\n",
    "    for i in range(len(series)):\n",
    "        if i > 0 and series.iloc[i-1] > 0:\n",
    "            last_sale_idx = i - 1\n",
    "        if last_sale_idx >= 0:\n",
    "            result[i] = min(i - last_sale_idx, 90)\n",
    "        else:\n",
    "            result[i] = 90\n",
    "    return result\n",
    "\n",
    "panel['days_since_last_sale'] = panel.groupby(['store_id', 'sku_id'])['y'].transform(\n",
    "    lambda x: pd.Series(days_since_last_sale(x), index=x.index)\n",
    ")\n",
    "\n",
    "# Zero run length (consecutive zeros)\n",
    "def zero_run_length(series):\n",
    "    result = np.zeros(len(series))\n",
    "    run = 0\n",
    "    for i in range(len(series)):\n",
    "        if i > 0:\n",
    "            if series.iloc[i-1] == 0:\n",
    "                run += 1\n",
    "            else:\n",
    "                run = 0\n",
    "        result[i] = min(run, 60)\n",
    "    return result\n",
    "\n",
    "panel['zero_run_length'] = panel.groupby(['store_id', 'sku_id'])['y'].transform(\n",
    "    lambda x: pd.Series(zero_run_length(x), index=x.index)\n",
    ")\n",
    "\n",
    "# Last sale quantity (capped)\n",
    "def last_sale_qty(series):\n",
    "    result = np.zeros(len(series))\n",
    "    last_qty = 0\n",
    "    for i in range(len(series)):\n",
    "        if i > 0 and series.iloc[i-1] > 0:\n",
    "            last_qty = min(series.iloc[i-1], 50)\n",
    "        result[i] = last_qty\n",
    "    return result\n",
    "\n",
    "panel['last_sale_qty'] = panel.groupby(['store_id', 'sku_id'])['y'].transform(\n",
    "    lambda x: pd.Series(last_sale_qty(x), index=x.index)\n",
    ")\n",
    "\n",
    "print(\"  ‚úì Dormancy features (4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === SPIKE FEATURES (Inferred Promotional Signals) ===\n",
    "print(\"  Computing spike features...\")\n",
    "\n",
    "# Historical mean for spike detection\n",
    "series_mean = panel[panel['y'] > 0].groupby(['store_id', 'sku_id'])['y'].mean().reset_index()\n",
    "series_mean.columns = ['store_id', 'sku_id', 'hist_mean']\n",
    "panel = panel.merge(series_mean, on=['store_id', 'sku_id'], how='left')\n",
    "panel['hist_mean'] = panel['hist_mean'].fillna(1)\n",
    "\n",
    "# Is this a spike day? (>3x historical mean)\n",
    "panel['is_spike'] = ((panel['y'] > 3 * panel['hist_mean']) & (panel['y'] > 5)).astype(int)\n",
    "\n",
    "# Store spike percentage (how many SKUs spiked in this store today)\n",
    "store_spike_pct = panel.groupby(['store_id', 'date'])['is_spike'].transform('mean')\n",
    "panel['store_spike_pct'] = store_spike_pct\n",
    "\n",
    "# Historical spike probability for this series\n",
    "panel['hist_spike_prob'] = panel.groupby(['store_id', 'sku_id'])['is_spike'].transform(\n",
    "    lambda x: x.shift(1).expanding().mean()\n",
    ").fillna(0)\n",
    "\n",
    "# Had recent spike (in last 7 days)\n",
    "panel['had_recent_spike'] = panel.groupby(['store_id', 'sku_id'])['is_spike'].transform(\n",
    "    lambda x: x.shift(1).rolling(7, min_periods=1).max()\n",
    ").fillna(0)\n",
    "\n",
    "# Clean up temporary columns\n",
    "panel = panel.drop(columns=['hist_mean', 'is_spike'])\n",
    "\n",
    "print(\"  ‚úì Spike features (3)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill any remaining NaNs\n",
    "feature_cols = [\n",
    "    'dow', 'is_weekend', 'week_of_year', 'month', 'day_of_year',\n",
    "    'sin_doy', 'cos_doy', 'sin_dow', 'cos_dow',\n",
    "    'lag_1', 'lag_7', 'lag_14', 'lag_28', 'lag_56',\n",
    "    'roll_mean_7', 'roll_sum_7', 'roll_mean_28', 'roll_sum_28', 'roll_std_28',\n",
    "    'nz_rate_28', 'days_since_last_sale', 'zero_run_length', 'last_sale_qty',\n",
    "    'store_spike_pct', 'hist_spike_prob', 'had_recent_spike',\n",
    "    'is_local'\n",
    "]\n",
    "\n",
    "for col in feature_cols:\n",
    "    if col in panel.columns:\n",
    "        panel[col] = panel[col].fillna(0)\n",
    "\n",
    "print(f\"\\n‚úì Feature engineering complete: {len(feature_cols)} features\")\n",
    "print(f\"  Panel shape: {panel.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ABC Segmentation <a name=\"8-segmentation\"></a>\n",
    "\n",
    "Segment series by sales volume: A (top 80%), B (next 15%), C (bottom 5%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Assigning ABC segments...\")\n",
    "\n",
    "# Calculate total sales per series\n",
    "series_sales = panel.groupby(['store_id', 'sku_id'])['y'].sum().reset_index()\n",
    "series_sales.columns = ['store_id', 'sku_id', 'total_sales']\n",
    "series_sales = series_sales.sort_values('total_sales', ascending=False)\n",
    "\n",
    "# Cumulative share\n",
    "total = series_sales['total_sales'].sum()\n",
    "series_sales['cum_share'] = series_sales['total_sales'].cumsum() / total\n",
    "\n",
    "# Assign segments\n",
    "series_sales['abc'] = 'C'\n",
    "series_sales.loc[series_sales['cum_share'] <= 0.80, 'abc'] = 'A'\n",
    "series_sales.loc[(series_sales['cum_share'] > 0.80) & (series_sales['cum_share'] <= 0.95), 'abc'] = 'B'\n",
    "\n",
    "# Merge to panel\n",
    "panel = panel.merge(series_sales[['store_id', 'sku_id', 'abc', 'total_sales']], \n",
    "                    on=['store_id', 'sku_id'], how='left')\n",
    "panel['abc'] = panel['abc'].fillna('C')\n",
    "\n",
    "# Summary\n",
    "print(\"\\nABC Distribution:\")\n",
    "for seg in ['A', 'B', 'C']:\n",
    "    n_series = (series_sales['abc'] == seg).sum()\n",
    "    sales_share = series_sales[series_sales['abc'] == seg]['total_sales'].sum() / total * 100\n",
    "    print(f\"  {seg}: {n_series:,} series ({sales_share:.1f}% of sales volume)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define behavior buckets for evaluation\n",
    "# Fast movers: top 20% by total sales\n",
    "# Slow movers: bottom 20% by total sales (but non-zero)\n",
    "# Intermittent: nz_rate < 0.05\n",
    "\n",
    "series_stats = panel.groupby(['store_id', 'sku_id']).agg({\n",
    "    'y': 'sum',\n",
    "    'nz_rate_28': 'mean'\n",
    "}).reset_index()\n",
    "series_stats.columns = ['store_id', 'sku_id', 'total_sales', 'avg_nz_rate']\n",
    "\n",
    "# Assign buckets\n",
    "sales_80 = series_stats['total_sales'].quantile(0.80)\n",
    "sales_20 = series_stats['total_sales'].quantile(0.20)\n",
    "\n",
    "series_stats['bucket'] = 'regular'\n",
    "series_stats.loc[series_stats['total_sales'] >= sales_80, 'bucket'] = 'fast_mover'\n",
    "series_stats.loc[(series_stats['total_sales'] <= sales_20) & (series_stats['total_sales'] > 0), 'bucket'] = 'slow_mover'\n",
    "series_stats.loc[series_stats['avg_nz_rate'] < 0.05, 'bucket'] = 'intermittent'\n",
    "\n",
    "# Merge\n",
    "panel = panel.merge(series_stats[['store_id', 'sku_id', 'bucket']], on=['store_id', 'sku_id'], how='left')\n",
    "\n",
    "print(\"\\nBehavior Buckets:\")\n",
    "print(series_stats['bucket'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Training <a name=\"9-training\"></a>\n",
    "\n",
    "Two-stage LightGBM with ABC-specific hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and hyperparameters\n",
    "FEATURES = [\n",
    "    'dow', 'is_weekend', 'week_of_year', 'month', 'day_of_year',\n",
    "    'sin_doy', 'cos_doy', 'sin_dow', 'cos_dow',\n",
    "    'lag_1', 'lag_7', 'lag_14', 'lag_28', 'lag_56',\n",
    "    'roll_mean_7', 'roll_sum_7', 'roll_mean_28', 'roll_sum_28', 'roll_std_28',\n",
    "    'nz_rate_28', 'days_since_last_sale', 'zero_run_length', 'last_sale_qty',\n",
    "    'store_spike_pct', 'hist_spike_prob', 'had_recent_spike',\n",
    "    'is_local'\n",
    "]\n",
    "CAT_FEATURES = ['store_id', 'sku_id']\n",
    "\n",
    "SEGMENT_PARAMS = {\n",
    "    'A': {'num_leaves': 255, 'learning_rate': 0.015, 'n_clf': 800, 'n_reg': 1000, 'min_data': 10, 'threshold': 0.6},\n",
    "    'B': {'num_leaves': 63, 'learning_rate': 0.03, 'n_clf': 300, 'n_reg': 400, 'min_data': 50, 'threshold': 0.6},\n",
    "    'C': {'num_leaves': 31, 'learning_rate': 0.05, 'n_clf': 200, 'n_reg': 300, 'min_data': 100, 'threshold': 0.7},\n",
    "}\n",
    "\n",
    "print(f\"Features: {len(FEATURES)} numeric + {len(CAT_FEATURES)} categorical\")\n",
    "print(\"\\nSegment hyperparameters:\")\n",
    "for seg, params in SEGMENT_PARAMS.items():\n",
    "    print(f\"  {seg}: leaves={params['num_leaves']}, lr={params['learning_rate']}, threshold={params['threshold']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_two_stage_model(train_df, segment, params, features, cat_features):\n",
    "    \"\"\"Train classifier + regressor for one segment. Returns models.\"\"\"\n",
    "    \n",
    "    train_seg = train_df[train_df['abc'] == segment].copy()\n",
    "    if len(train_seg) < 100:\n",
    "        return None, None\n",
    "    \n",
    "    # Prepare data\n",
    "    train_seg['y_binary'] = (train_seg['y'] > 0).astype(int)\n",
    "    for col in cat_features:\n",
    "        train_seg[col] = train_seg[col].astype('category')\n",
    "    \n",
    "    X_train = train_seg[features + cat_features]\n",
    "    \n",
    "    # Stage 1: Classifier\n",
    "    clf_params = {\n",
    "        'objective': 'binary', 'metric': 'auc',\n",
    "        'num_leaves': params['num_leaves'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'feature_fraction': 0.8,\n",
    "        'min_data_in_leaf': params['min_data'],\n",
    "        'verbose': -1, 'n_jobs': -1, 'seed': RANDOM_SEED\n",
    "    }\n",
    "    clf_data = lgb.Dataset(X_train, label=train_seg['y_binary'], categorical_feature=cat_features)\n",
    "    clf = lgb.train(clf_params, clf_data, num_boost_round=params['n_clf'])\n",
    "    \n",
    "    # Stage 2: Regressor (non-zero only)\n",
    "    train_nz = train_seg[train_seg['y'] > 0]\n",
    "    if len(train_nz) < 10:\n",
    "        return clf, None\n",
    "    \n",
    "    X_train_nz = train_nz[features + cat_features]\n",
    "    y_train_nz = np.log1p(train_nz['y'].values)\n",
    "    \n",
    "    reg_params = {\n",
    "        'objective': 'regression_l1', 'metric': 'mae',\n",
    "        'num_leaves': params['num_leaves'],\n",
    "        'learning_rate': params['learning_rate'],\n",
    "        'feature_fraction': 0.8,\n",
    "        'min_data_in_leaf': max(5, params['min_data'] // 2),\n",
    "        'lambda_l2': 0.5,\n",
    "        'verbose': -1, 'n_jobs': -1, 'seed': RANDOM_SEED\n",
    "    }\n",
    "    reg_data = lgb.Dataset(X_train_nz, label=y_train_nz, categorical_feature=cat_features)\n",
    "    reg = lgb.train(reg_params, reg_data, num_boost_round=params['n_reg'])\n",
    "    \n",
    "    return clf, reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models for each segment\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING TWO-STAGE MODELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "models = {}\n",
    "\n",
    "for seg in ['A', 'B', 'C']:\n",
    "    seg_data = panel[panel['abc'] == seg]\n",
    "    print(f\"\\n{seg}-items: {len(seg_data):,} rows\")\n",
    "    \n",
    "    clf, reg = train_two_stage_model(panel, seg, SEGMENT_PARAMS[seg], FEATURES, CAT_FEATURES)\n",
    "    models[seg] = {'clf': clf, 'reg': reg, 'params': SEGMENT_PARAMS[seg]}\n",
    "    \n",
    "    if clf is not None:\n",
    "        print(f\"  ‚úì Classifier trained\")\n",
    "    if reg is not None:\n",
    "        print(f\"  ‚úì Regressor trained\")\n",
    "\n",
    "print(\"\\n‚úì All models trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Forecast Generation <a name=\"10-forecast\"></a>\n",
    "\n",
    "Generate 168-day forecasts starting from December 18, 2025."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Preparing forecast period...\")\n",
    "\n",
    "# Generate forecast dates\n",
    "forecast_start = pd.to_datetime(FORECAST_START)\n",
    "forecast_dates = pd.date_range(forecast_start, periods=HORIZON_DAYS, freq='D')\n",
    "\n",
    "print(f\"  Forecast period: {forecast_dates[0].date()} to {forecast_dates[-1].date()}\")\n",
    "print(f\"  Days: {len(forecast_dates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecast panel (store-SKU √ó dates)\n",
    "print(\"\\nBuilding forecast panel...\")\n",
    "\n",
    "series_list = panel[['store_id', 'sku_id', 'abc', 'bucket', 'is_local']].drop_duplicates()\n",
    "print(f\"  Series: {len(series_list):,}\")\n",
    "\n",
    "# Cross join with dates\n",
    "series_list['_key'] = 1\n",
    "dates_df = pd.DataFrame({'date': forecast_dates})\n",
    "dates_df['_key'] = 1\n",
    "forecast_panel = series_list.merge(dates_df, on='_key').drop('_key', axis=1)\n",
    "\n",
    "print(f\"  Forecast panel: {len(forecast_panel):,} rows\")\n",
    "print(f\"  Expected: {len(series_list):,} √ó {len(forecast_dates)} = {len(series_list) * len(forecast_dates):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add calendar features\n",
    "forecast_panel['dow'] = forecast_panel['date'].dt.dayofweek\n",
    "forecast_panel['is_weekend'] = forecast_panel['dow'].isin([5, 6]).astype(int)\n",
    "forecast_panel['week_of_year'] = forecast_panel['date'].dt.isocalendar().week.astype(int)\n",
    "forecast_panel['month'] = forecast_panel['date'].dt.month\n",
    "forecast_panel['day_of_year'] = forecast_panel['date'].dt.dayofyear\n",
    "forecast_panel['sin_doy'] = np.sin(2 * np.pi * forecast_panel['day_of_year'] / 365)\n",
    "forecast_panel['cos_doy'] = np.cos(2 * np.pi * forecast_panel['day_of_year'] / 365)\n",
    "forecast_panel['sin_dow'] = np.sin(2 * np.pi * forecast_panel['dow'] / 7)\n",
    "forecast_panel['cos_dow'] = np.cos(2 * np.pi * forecast_panel['dow'] / 7)\n",
    "\n",
    "print(\"  ‚úì Calendar features added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get last known values from training data for lag/rolling features\n",
    "print(\"\\nComputing lag features from last known values...\")\n",
    "\n",
    "# Get last N days of training data per series\n",
    "cutoff = pd.to_datetime(CUTOFF_DATE)\n",
    "lookback = panel[panel['date'] > cutoff - timedelta(days=60)].copy()\n",
    "\n",
    "# Compute summary stats per series\n",
    "series_last_stats = lookback.groupby(['store_id', 'sku_id']).agg({\n",
    "    'y': ['last', 'mean', 'sum', 'std'],\n",
    "    'nz_rate_28': 'last',\n",
    "    'days_since_last_sale': 'last',\n",
    "    'zero_run_length': 'last',\n",
    "    'last_sale_qty': 'last',\n",
    "    'store_spike_pct': 'mean',\n",
    "    'hist_spike_prob': 'last',\n",
    "    'had_recent_spike': 'last'\n",
    "}).reset_index()\n",
    "\n",
    "series_last_stats.columns = ['store_id', 'sku_id', \n",
    "                              'lag_1', 'roll_mean_28', 'roll_sum_28', 'roll_std_28',\n",
    "                              'nz_rate_28', 'days_since_last_sale', 'zero_run_length', 'last_sale_qty',\n",
    "                              'store_spike_pct', 'hist_spike_prob', 'had_recent_spike']\n",
    "\n",
    "# Fill derived features\n",
    "series_last_stats['lag_7'] = series_last_stats['lag_1']\n",
    "series_last_stats['lag_14'] = series_last_stats['lag_1'] \n",
    "series_last_stats['lag_28'] = series_last_stats['lag_1']\n",
    "series_last_stats['lag_56'] = series_last_stats['lag_1']\n",
    "series_last_stats['roll_mean_7'] = series_last_stats['roll_mean_28']\n",
    "series_last_stats['roll_sum_7'] = series_last_stats['roll_sum_28'] / 4\n",
    "\n",
    "# Merge to forecast panel\n",
    "forecast_panel = forecast_panel.merge(series_last_stats, on=['store_id', 'sku_id'], how='left')\n",
    "\n",
    "# Fill any missing values\n",
    "for col in FEATURES:\n",
    "    if col in forecast_panel.columns:\n",
    "        forecast_panel[col] = forecast_panel[col].fillna(0)\n",
    "    else:\n",
    "        forecast_panel[col] = 0\n",
    "\n",
    "print(\"  ‚úì Features prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "\n",
    "forecast_panel['predicted_sales'] = 0.0\n",
    "\n",
    "for seg in ['A', 'B', 'C']:\n",
    "    seg_mask = forecast_panel['abc'] == seg\n",
    "    seg_data = forecast_panel[seg_mask].copy()\n",
    "    \n",
    "    if len(seg_data) == 0:\n",
    "        continue\n",
    "    \n",
    "    clf = models[seg]['clf']\n",
    "    reg = models[seg]['reg']\n",
    "    threshold = models[seg]['params']['threshold']\n",
    "    \n",
    "    if clf is None:\n",
    "        continue\n",
    "    \n",
    "    # Prepare features\n",
    "    for col in CAT_FEATURES:\n",
    "        seg_data[col] = seg_data[col].astype('category')\n",
    "    \n",
    "    X = seg_data[FEATURES + CAT_FEATURES]\n",
    "    \n",
    "    # Predict\n",
    "    prob = clf.predict(X)\n",
    "    \n",
    "    if reg is not None:\n",
    "        pred_value = np.expm1(reg.predict(X))\n",
    "    else:\n",
    "        pred_value = np.ones(len(X))\n",
    "    \n",
    "    # Combine\n",
    "    y_pred = np.where(prob > threshold, pred_value, 0)\n",
    "    y_pred = np.maximum(0, y_pred)\n",
    "    \n",
    "    forecast_panel.loc[seg_mask, 'predicted_sales'] = y_pred\n",
    "    \n",
    "    print(f\"  {seg}: {len(seg_data):,} rows, avg pred = {y_pred.mean():.2f}\")\n",
    "\n",
    "print(\"\\n‚úì Predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Evaluation & Sanity Checks <a name=\"11-evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SANITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_series = forecast_panel.groupby(['store_id', 'sku_id']).ngroups\n",
    "expected_rows = n_series * HORIZON_DAYS\n",
    "actual_rows = len(forecast_panel)\n",
    "\n",
    "print(f\"\\n1. Row count:\")\n",
    "print(f\"   Series: {n_series:,}\")\n",
    "print(f\"   Expected rows: {n_series:,} √ó {HORIZON_DAYS} = {expected_rows:,}\")\n",
    "print(f\"   Actual rows: {actual_rows:,}\")\n",
    "print(f\"   ‚úì Match\" if expected_rows == actual_rows else f\"   ‚úó Mismatch!\")\n",
    "\n",
    "print(f\"\\n2. Date range:\")\n",
    "print(f\"   Min: {forecast_panel['date'].min().date()}\")\n",
    "print(f\"   Max: {forecast_panel['date'].max().date()}\")\n",
    "print(f\"   Days: {forecast_panel['date'].nunique()}\")\n",
    "print(f\"   ‚úì Correct\" if forecast_panel['date'].nunique() == HORIZON_DAYS else f\"   ‚úó Wrong!\")\n",
    "\n",
    "print(f\"\\n3. Predictions quality:\")\n",
    "neg_preds = (forecast_panel['predicted_sales'] < 0).sum()\n",
    "nan_preds = forecast_panel['predicted_sales'].isna().sum()\n",
    "print(f\"   Negative predictions: {neg_preds}\")\n",
    "print(f\"   NaN predictions: {nan_preds}\")\n",
    "print(f\"   ‚úì Clean\" if neg_preds == 0 and nan_preds == 0 else f\"   ‚úó Issues!\")\n",
    "\n",
    "print(f\"\\n4. Prediction distribution:\")\n",
    "print(f\"   Mean: {forecast_panel['predicted_sales'].mean():.3f}\")\n",
    "print(f\"   Median: {forecast_panel['predicted_sales'].median():.3f}\")\n",
    "print(f\"   Max: {forecast_panel['predicted_sales'].max():.1f}\")\n",
    "print(f\"   % zeros: {(forecast_panel['predicted_sales'] == 0).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics by behavior bucket\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTIONS BY BEHAVIOR BUCKET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for bucket in ['fast_mover', 'regular', 'slow_mover', 'intermittent']:\n",
    "    bucket_data = forecast_panel[forecast_panel['bucket'] == bucket]\n",
    "    if len(bucket_data) > 0:\n",
    "        avg_pred = bucket_data['predicted_sales'].mean()\n",
    "        zero_rate = (bucket_data['predicted_sales'] == 0).mean() * 100\n",
    "        print(f\"  {bucket:15} | Series: {bucket_data.groupby(['store_id', 'sku_id']).ngroups:>6,} | \"\n",
    "              f\"Avg pred: {avg_pred:>6.2f} | Zero%: {zero_rate:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics by Local vs Import\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTIONS BY LOCAL VS IMPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for is_local, label in [(1, 'Local'), (0, 'Import')]:\n",
    "    subset = forecast_panel[forecast_panel['is_local'] == is_local]\n",
    "    if len(subset) > 0:\n",
    "        avg_pred = subset['predicted_sales'].mean()\n",
    "        zero_rate = (subset['predicted_sales'] == 0).mean() * 100\n",
    "        print(f\"  {label:10} | Series: {subset.groupby(['store_id', 'sku_id']).ngroups:>6,} | \"\n",
    "              f\"Avg pred: {avg_pred:>6.2f} | Zero%: {zero_rate:>5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample plots: 6 series (2 fast, 2 intermittent, 2 cold-start)\n",
    "print(\"\\nGenerating sample forecast plots...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Get sample series\n",
    "sample_series = []\n",
    "\n",
    "# Fast movers (2)\n",
    "fast = series_list[series_list['bucket'] == 'fast_mover'].head(2)\n",
    "for _, row in fast.iterrows():\n",
    "    sample_series.append((row['store_id'], row['sku_id'], 'Fast Mover'))\n",
    "\n",
    "# Intermittent (2)\n",
    "inter = series_list[series_list['bucket'] == 'intermittent'].head(2)\n",
    "for _, row in inter.iterrows():\n",
    "    sample_series.append((row['store_id'], row['sku_id'], 'Intermittent'))\n",
    "\n",
    "# Slow movers (2)\n",
    "slow = series_list[series_list['bucket'] == 'slow_mover'].head(2)\n",
    "for _, row in slow.iterrows():\n",
    "    sample_series.append((row['store_id'], row['sku_id'], 'Slow Mover'))\n",
    "\n",
    "for idx, (store, sku, label) in enumerate(sample_series[:6]):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Historical (last 120 days)\n",
    "    hist = panel[(panel['store_id'] == store) & (panel['sku_id'] == sku)].tail(120)\n",
    "    \n",
    "    # Forecast\n",
    "    fcast = forecast_panel[(forecast_panel['store_id'] == store) & (forecast_panel['sku_id'] == sku)]\n",
    "    \n",
    "    if len(hist) > 0:\n",
    "        ax.plot(hist['date'], hist['y'], 'b-', alpha=0.7, label='Historical')\n",
    "    if len(fcast) > 0:\n",
    "        ax.plot(fcast['date'], fcast['predicted_sales'], 'r--', alpha=0.7, label='Forecast')\n",
    "    \n",
    "    ax.axvline(pd.to_datetime(CUTOFF_DATE), color='gray', linestyle=':', label='Cutoff')\n",
    "    ax.set_title(f'{label}\\nStore {store}, SKU {sku}')\n",
    "    ax.legend(loc='upper left', fontsize=8)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Output & Submission Checklist <a name=\"12-output\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare output in required format\n",
    "print(\"Preparing output file...\")\n",
    "\n",
    "output = forecast_panel[['sku_id', 'store_id', 'date', 'predicted_sales']].copy()\n",
    "output = output.rename(columns={'sku_id': 'item_id'})\n",
    "output['date'] = output['date'].dt.strftime('%Y-%m-%d')\n",
    "output['predicted_sales'] = output['predicted_sales'].round(2)\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(os.path.dirname(OUTPUT_PATH) if os.path.dirname(OUTPUT_PATH) else '.', exist_ok=True)\n",
    "\n",
    "# Save\n",
    "output.to_csv(OUTPUT_PATH, index=False)\n",
    "file_size = os.path.getsize(OUTPUT_PATH) / 1e6\n",
    "\n",
    "print(f\"\\n‚úì Saved to {OUTPUT_PATH}\")\n",
    "print(f\"  Rows: {len(output):,}\")\n",
    "print(f\"  Size: {file_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview output\n",
    "print(\"\\nOutput preview:\")\n",
    "print(output.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SUBMISSION CHECKLIST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "checks = [\n",
    "    (\"Output file exists\", os.path.exists(OUTPUT_PATH)),\n",
    "    (\"Columns: item_id, store_id, date, predicted_sales\", list(output.columns) == ['item_id', 'store_id', 'date', 'predicted_sales']),\n",
    "    (f\"Row count = series √ó {HORIZON_DAYS}\", len(output) == n_series * HORIZON_DAYS),\n",
    "    (\"Date range: 168 days\", output['date'].nunique() == HORIZON_DAYS),\n",
    "    (\"No negative predictions\", (output['predicted_sales'] >= 0).all()),\n",
    "    (\"No NaN predictions\", output['predicted_sales'].notna().all()),\n",
    "    (f\"Start date: {FORECAST_START}\", output['date'].min() == FORECAST_START),\n",
    "]\n",
    "\n",
    "all_pass = True\n",
    "for check, passed in checks:\n",
    "    status = \"‚úì\" if passed else \"‚úó\"\n",
    "    print(f\"  [{status}] {check}\")\n",
    "    if not passed:\n",
    "        all_pass = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "if all_pass:\n",
    "    print(\"‚úì ALL CHECKS PASSED - READY FOR SUBMISSION\")\n",
    "else:\n",
    "    print(\"‚úó SOME CHECKS FAILED - REVIEW BEFORE SUBMISSION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nNotebook completed: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
